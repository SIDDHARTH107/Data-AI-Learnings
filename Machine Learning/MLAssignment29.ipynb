{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n",
        "\n",
        "The main difference between the Euclidean distance metric and the Manhattan distance metric is that the Euclidean distance takes into account the magnitude of the differences between the two points, while the Manhattan distance only takes into account the absolute values of the differences. This difference can affect the performance of a KNN classifier or regressor in a number of ways. For example, if the features in the dataset are all on the same scale, then the Euclidean distance and the Manhattan distance will be equivalent. However, if the features are on different scales, then the Euclidean distance will be more sensitive to the differences between the points, while the Manhattan distance will be less sensitive.\n",
        "\n",
        "In general, the Euclidean distance is a more sensitive metric than the Manhattan distance. This means that the Euclidean distance will be more likely to find similar points that are far apart in terms of the Manhattan distance. This can be beneficial in some cases, such as when the features in the dataset are on different scales. However, it can also be a drawback in other cases, such as when the dataset contains a lot of noise.\n",
        "\n",
        "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?\n",
        "\n",
        "The optimal value of k for a KNN classifier or regressor depends on the dataset and the task at hand. There is no one-size-fits-all answer, and the best way to choose the value of k is to experiment with different values and see what works best.\n",
        "\n",
        "One technique that can be used to determine the optimal value of k is to use a cross-validation procedure. Cross-validation involves dividing the dataset into a training set and a test set. The training set is used to train the KNN model, and the test set is used to evaluate the performance of the model. This process is repeated for different values of k, and the value of k that results in the best performance on the test set is chosen as the optimal value of k.\n",
        "\n",
        "Another technique that can be used to determine the optimal value of k is to use a grid search. Grid search involves trying all possible combinations of k and evaluating the performance of the model for each combination. The combination of k that results in the best performance is chosen as the optimal value of k.\n",
        "\n",
        "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?\n",
        "\n",
        "The choice of distance metric can have a significant impact on the performance of a KNN classifier or regressor. In general, the Euclidean distance is a more sensitive metric than the Manhattan distance. This means that the Euclidean distance will be more likely to find similar points that are far apart in terms of the Manhattan distance. This can be beneficial in some cases, such as when the features in the dataset are on different scales. However, it can also be a drawback in other cases, such as when the dataset contains a lot of noise.\n",
        "\n",
        "The Manhattan distance is less sensitive to noise than the Euclidean distance. This means that the Manhattan distance is less likely to be affected by outliers in the dataset. This can be beneficial in cases where the dataset contains a lot of noise.\n",
        "\n",
        "In general, the best choice of distance metric depends on the specific dataset and the task at hand. If the features in the dataset are on different scales, then the Euclidean distance may be a better choice. If the dataset contains a lot of noise, then the Manhattan distance may be a better choice.\n",
        "\n",
        "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?\n",
        "\n",
        "Some common hyperparameters in KNN classifiers and regressors include:\n",
        "\n",
        "The value of k\n",
        "The choice of distance metric\n",
        "The presence of feature scaling\n",
        "The value of k controls the number of neighbors that are used to make a prediction. The choice of distance metric affects how the distance between two points is measured. Feature scaling can help to improve the performance of KNN by making it easier for the algorithm to find the k most similar instances in the training set.\n",
        "\n",
        "The best way to tune these hyperparameters is to experiment with different values and see what works best. One technique that can be used to tune hyperparameters is to use a grid search. Grid search involves trying all possible combinations of hyperparameters and evaluating the performance of the model for each combination. The combination of hyperparameters that results in the best performance is chosen as the optimal combination of hyperparameters."
      ],
      "metadata": {
        "id": "NcyTyoK-zUpC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "po_mDPhfzxIf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}