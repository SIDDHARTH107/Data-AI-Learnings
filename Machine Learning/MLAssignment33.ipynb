{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Eigenvalues and eigenvectors are concepts in linear algebra.\n",
        "\n",
        "Eigenvalues are numbers that represent how a linear transformation (associated with a matrix) stretches or shrinks space along its eigenvectors. They tell us how much the eigenvectors are scaled.\n",
        "Eigenvectors are non-zero vectors that remain in the same direction after the linear transformation by the matrix. They provide the directions along which the linear transformation behaves in a simple way.\n",
        "In the Eigen-Decomposition approach, a matrix is decomposed into a product of its eigenvectors and eigenvalues. It's expressed as A = PDP^(-1), where A is the matrix, P is the matrix of eigenvectors, D is the diagonal matrix of eigenvalues, and P^(-1) is the inverse of the matrix P.\n",
        "\n",
        "Example: Let's say we have a 2x2 matrix A:\n",
        "\n",
        "A = | 2 1 |\n",
        "| 1 3 |\n",
        "\n",
        "By finding its eigenvalues and eigenvectors, we can represent A as a product of these:\n",
        "\n",
        "Eigenvalues (λ): λ1 = 4, λ2 = 1\n",
        "Eigenvectors (v): v1 = [1, 1], v2 = [-1, 1]\n",
        "\n",
        "So, A = PDP^(-1) can be written as:\n",
        "\n",
        "A = | 1 -1 | | 4 0 | | 1 1 |\n",
        "| 1 1 | | 0 1 | | 1 -1 |\n",
        "\n",
        "Q2. Eigen decomposition is a process in linear algebra where a square matrix is decomposed into a set of eigenvalues and eigenvectors. It's significant because it simplifies complex matrix operations and helps understand the behavior of the matrix transformation.\n",
        "\n",
        "Q3. For a square matrix to be diagonalizable using the Eigen-Decomposition approach, it must have a full set of linearly independent eigenvectors. A brief proof involves verifying that the matrix P (containing eigenvectors) is invertible.\n",
        "\n",
        "Q4. The spectral theorem states that for a real symmetric matrix, all eigenvalues are real, and the corresponding eigenvectors are orthogonal (perpendicular to each other). It's related to diagonalizability because it ensures that a real symmetric matrix can be diagonalized by an orthogonal matrix P.\n",
        "\n",
        "Example: A symmetric matrix like a covariance matrix in statistics is often diagonalized using the spectral theorem.\n",
        "\n",
        "Q5. Eigenvalues of a matrix are found by solving the characteristic equation det(A - λI) = 0, where A is the matrix, λ is the eigenvalue, and I is the identity matrix. Eigenvalues represent how the matrix scales space along its eigenvectors.\n",
        "\n",
        "Q6. Eigenvectors are vectors that don't change direction when multiplied by a matrix. They're related to eigenvalues because each eigenvalue corresponds to a unique eigenvector. Eigenvalues determine how much the eigenvectors are scaled.\n",
        "\n",
        "Q7. The geometric interpretation of eigenvectors and eigenvalues is that eigenvectors represent the directions along which a linear transformation stretches or shrinks space, and eigenvalues represent how much it stretches or shrinks along those directions.\n",
        "\n",
        "Q8. Real-world applications of eigen decomposition include:\n",
        "\n",
        "Principal Component Analysis (PCA) in dimensionality reduction and data compression.\n",
        "Vibrational analysis in structural engineering.\n",
        "Quantum mechanics in solving Schrödinger's equation.\n",
        "Q9. Yes, a matrix can have more than one set of eigenvectors and eigenvalues if it is not diagonalizable. For example, a non-square matrix or a matrix with repeated eigenvalues may not have a full set of linearly independent eigenvectors.\n",
        "\n",
        "Q10. Eigen-Decomposition is useful in data analysis and machine learning in various ways:\n",
        "\n",
        "PCA for dimensionality reduction and feature extraction.\n",
        "Image compression and face recognition.\n",
        "Solving linear systems and differential equations in scientific simulations."
      ],
      "metadata": {
        "id": "Yjkp1Pawv3ND"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JEGE5NDAxg2z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}