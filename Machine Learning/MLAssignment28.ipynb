{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is the KNN algorithm?\n",
        "\n",
        "KNN stands for k-nearest neighbors. It is a non-parametric supervised learning algorithm that can be used for both classification and regression tasks. The KNN algorithm works by finding the k most similar instances in the training set to a new instance, and then predicting the label or value of the new instance based on the labels or values of the k nearest neighbors.\n",
        "\n",
        "Q2. How do you choose the value of K in KNN?\n",
        "\n",
        "The value of K in KNN is a hyperparameter that controls the number of neighbors that are used to make a prediction. The optimal value of K depends on the dataset and the task at hand. There is no one-size-fits-all answer, and the best way to choose the value of K is to experiment with different values and see what works best.\n",
        "\n",
        "Q3. What is the difference between KNN classifier and KNN regressor?\n",
        "\n",
        "The main difference between KNN classifier and KNN regressor is the way they make predictions. The KNN classifier predicts the class label of a new instance by finding the k most similar instances in the training set and then predicting the label of the new instance based on the majority class of the k nearest neighbors. The KNN regressor predicts the value of a continuous variable for a new instance by finding the k most similar instances in the training set and then averaging the values of the k nearest neighbors.\n",
        "\n",
        "Q4. How do you measure the performance of KNN?\n",
        "\n",
        "The performance of KNN can be measured using a variety of metrics, such as accuracy, precision, recall, and F1 score. The choice of metric depends on the specific task at hand. For example, accuracy is a good metric for classification tasks, while precision and recall are good metrics for binary classification tasks.\n",
        "\n",
        "Q5. What is the curse of dimensionality in KNN?\n",
        "\n",
        "The curse of dimensionality is a phenomenon that occurs when the number of features in a dataset is very large. This can make it difficult for KNN to find the k most similar instances in the training set, and can lead to decreased performance. There are a number of techniques that can be used to address the curse of dimensionality, such as feature selection and dimensionality reduction.\n",
        "\n",
        "Q6. How do you handle missing values in KNN?\n",
        "\n",
        "There are a number of ways to handle missing values in KNN. One common approach is to simply ignore instances with missing values. Another approach is to impute the missing values with some value, such as the mean or median of the feature. The best approach to handling missing values depends on the specific dataset and the task at hand.\n",
        "\n",
        "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
        "\n",
        "The KNN classifier and regressor have different strengths and weaknesses. The KNN classifier is typically better at handling small datasets and noisy data. The KNN regressor is typically better at handling large datasets and continuous variables.\n",
        "\n",
        "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
        "\n",
        "The KNN algorithm is a simple and intuitive algorithm that is easy to understand and implement. It is also a very versatile algorithm that can be used for a variety of classification and regression tasks. However, the KNN algorithm can be computationally expensive, and it can be sensitive to the choice of hyperparameters.\n",
        "\n",
        "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
        "\n",
        "The Euclidean distance and Manhattan distance are two different metrics that can be used to measure the distance between two points in a multidimensional space. The Euclidean distance is the length of the hypotenuse of a right triangle, while the Manhattan distance is the sum of the absolute values of the differences between the two points.\n",
        "\n",
        "Q10. What is the role of feature scaling in KNN?\n",
        "\n",
        "Feature scaling is the process of normalizing the features in a dataset so that they have the same scale. This can help to improve the performance of KNN by making it easier for the algorithm to find the k most similar instances in the training set."
      ],
      "metadata": {
        "id": "NcyTyoK-zUpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "+"
      ],
      "metadata": {
        "id": "po_mDPhfzxIf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}